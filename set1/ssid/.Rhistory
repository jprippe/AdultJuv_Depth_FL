require(ncdf4)
require(raster)
require(fields)
require(zoo)
require(xts)
Xmin<- -81.5 + 360
Xmax<- -78.5 + 360
Ymin<-23.5
Ymax<-26.5
start.date<-"1984/01/01"
end.date<-"2015/12/31"
start.date<-as.POSIXct(start.date,tz="UTC")
end.date<-as.POSIXct(end.date,tz="UTC")
#export folder
export.folder<-"~/Google Drive/Work/~Research/~Projects_Active/CaribCoring_NSF/Data/CloudData/"
#raw data file
raw.data<-"~/Google Drive/Work/~Research/~Projects_Active/CaribCoring_NSF/Data/CloudData/ISCCP-H_Aggregation_Basic_Gridded_Monthly_(HGM)_best.ncd.nc"
#function to find the probability given the correlation coeficient (R) and the number of observations (N)
r.to.sign<-function(R,N)
{
T<-R/(sqrt((1-(R^2))/(N-2)))
return((1-pt(T,df=N-2))*2)
}
#function to calculate the effective size of an autocorrelated dataseries (quenouille correction)
eff.sample<-function(corr.coef,N)
{
return(round(N*((1-corr.coef)/(1+corr.coef)),0))
}
#get dims from netcdf
ncdf<-nc_open(raw.data)
detach("package:ggmap", unload=TRUE)
library(rgeos)
library(rgdal)
library(maps)
library(mapdata)
library(maptools)
library(scales) #for transparency
library(GISTools)
library(devtools)
install.packages("gpclib", type="source")
library(mapproj)
library(dplyr)
library(ggplot2)
library(ggmap)
detach("package:ggmap", unload=TRUE)
library(rgeos)
library(rgdal)
library(maps)
library(mapdata)
library(maptools)
library(scales) #for transparency
library(GISTools)
library(devtools)
install.packages('png')
library(ggmap)
detach("package:ggmap", unload=TRUE)
library(rgeos)
library(rgdal)
library(maps)
library(mapdata)
library(maptools)
library(scales) #for transparency
library(GISTools)
library(devtools)
if (!rgeosStatus()) gpclibPermit()
gshhs.f.b <- "~/Downloads/gshhs_f.b"
sf1 <- getRgshhsMap(gshhs.f.b, xlim = c(-89, -87), ylim = c(16, 18)) %>%
fortify()
#import GSHHS coastline
require(maptools)
Coast<-getRgshhsMap('~/Downloads/gshhg-bin-2.3.7/gshhs_f.b')
require(ncdf4)
require(raster)
require(fields)
require(zoo)
require(xts)
Sys.setenv(TZ="UTC")
Xmin<- -81.5 + 360
Xmax<- -78.5 + 360
Ymin<-23.5
Ymax<-26.5
start.date<-"1984/01/01"
end.date<-"2015/12/31"
start.date<-as.POSIXct(start.date,tz="UTC")
end.date<-as.POSIXct(end.date,tz="UTC")
#export folder
export.folder<-"~/Google Drive/Work/~Research/~Projects_Active/CaribCoring_NSF/Data/CloudData/"
#raw data file
raw.data<-"~/Google Drive/Work/~Research/~Projects_Active/CaribCoring_NSF/Data/CloudData/ISCCP-H_Aggregation_Basic_Gridded_Monthly_(HGM)_best.ncd.nc"
#function to find the probability given the correlation coeficient (R) and the number of observations (N)
r.to.sign<-function(R,N)
{
T<-R/(sqrt((1-(R^2))/(N-2)))
return((1-pt(T,df=N-2))*2)
}
#function to calculate the effective size of an autocorrelated dataseries (quenouille correction)
eff.sample<-function(corr.coef,N)
{
return(round(N*((1-corr.coef)/(1+corr.coef)),0))
}
#get dims from netcdf
ncdf<-nc_open(raw.data)
nc.lats<-ncdf$dim$lat$vals
nc.longs<-ncdf$dim$lon$vals
nc.longs[nc.longs>180]<- nc.longs[nc.longs>180]-360
raw.times<-ncdf$dim$time$vals #times are seconds after the "start of time"
start.of.time<-ncdf$dim$time$units #here's what they consider the start of time
nc.times<-as.POSIXct(raw.times*3600,tz="UTC", gsub("hours since ","",start.of.time))
nc_close(ncdf)
#open netcdf as rasterbrick and clip it
data.brick<-brick(raw.data, lvar = 4, varname = 'cldamt')
data.brick<-crop(data.brick, extent(Xmin,Xmax,Ymin,Ymax))
#restrict temporally
Tstart.index<-min(which(difftime(nc.times,start.date)>0))
Tend.index<-min(which(difftime(nc.times,end.date)>0))
data.brick<-data.brick[[Tstart.index:Tend.index]]
site.raw.data<-read.csv("~/Google Drive/Work/~Research/~Projects_Active/CaribCoring_NSF/Data/Mapping/SamplingSites/SitePoints.csv")
site.raw.data<-site.raw.data[site.raw.data$Region=='Florida',]
#long and lat as a two column matrix
LongLat<-cbind(site.raw.data$Longitude + 360,site.raw.data$Latitude)
#converting sampling points to spatial points
sampling.points<-SpatialPoints(LongLat)
#create projection
lat.long.proj<-"+proj=longlat +ellps=WGS84 +datum=WGS84"
#apply projection to points
projection(sampling.points)<-lat.long.proj
first.day<-data.brick[[1]]
plot(first.day, col=tim.colors(255,1), xlab="Long", ylab="Lat", main=paste0("Cloudiness of Florida Keys on ",start.date), colNA="grey", cex.main=0.8, cex.axis=0.8, cex.lab=0.7)
#some pixels fall on land. Finding nearest valid pixels
#pixels coordinates
pixel.coordinates<-coordinates(first.day)
#delete coordinates where data is missing (NA)
#first, extract data for all pixels
data<-extract(first.day,pixel.coordinates)
#now check which ones are NA and delete those entries from the "pixel.coordinates" list
pixel.coordinates<-pixel.coordinates[!is.na(data),]
#create new sampling points, shifted to the center of the pixel they fall on, or to the near pixel in case that one is NA
sampling.points.shifted<-vector()
#loop cycling through sampling points
for (p in 1:length(sampling.points)) {
#geodesic distances
distances<-rdist.earth(coordinates(sampling.points[p]),pixel.coordinates,miles=FALSE)
#coordinates of the pixel that is at shorter distance
shifted.coordinates<-pixel.coordinates[which.min(distances),]
#assigning those coordinates to a new item
cmd<-paste0("sp.",p,"<-shifted.coordinates")
eval(parse(text=cmd))
#points(pt[1],pt[2])
cmd<-paste0("sampling.points.shifted<-rbind(sampling.points.shifted,sp.",p,")")
eval(parse(text=cmd))
}
coastline<-getRgshhsMap("~/Google\ Drive/Work/~Research/~Projects_Active/CaribCoring_NSF/Data/Mapping/gshhg-bin-2.3.6/gshhs_f.b", xlim=c(extent(data.brick)[1], extent(data.brick)[2]), ylim=c(extent(data.brick)[3], extent(data.brick)[4]), level=4, shift=TRUE, no.clip=FALSE)
gshhs.f.b <- "~/Downloads/gshhs_f.b"
sf1 <- getRgshhsMap(gshhs.f.b, xlim = c(-89, -87), ylim = c(16, 18))
#import and plot GSHHS coastline
require(maptools)
coastline<-getRgshhsMap('~/Downloads/gshhs_f.b')
coastline<-getRgshhsMap('~/Downloads/gshhs_f.b', xlim = c(-81,-79), ylim = c(24, 26))
read.table('~/Downloads/gshhs_f.b')
read.table('~/Downloads/gshhs_f.b')
read.table('~/Downloads/gshhs_f.txt')
read.table('~/Downloads/gshhg-bin-2.3.7/gshhs_f.b')
gshhs.f.b <- "~/Downloads/gshhg-bin-2.3.7/gshhs_f.b"
sf1 <- getRgshhsMap(gshhs.f.b, xlim = c(-89, -87), ylim = c(16, 18))
head(gshhs.f.b)
head(read.table(gshhs.f.b))
gshhs.f.b <- "~/Downloads/gshhg-bin-2.3.0/gshhs_f.b"
sf1 <- getRgshhsMap(gshhs.f.b, xlim = c(-89, -87), ylim = c(16, 18))
install.packages('sf')
require(sf)
coastline<-st_read("~/Downloads/gshhg-shp-2.3.7/GSHHS_shp/f/GSHHS_f_L4.shp")
st_crs(coastline)
coastline <- file("~/Downloads/gshhg-bin-2.3.7/gshhs_f.b", "rb")
test <- getRgshhsMap(coastline)
test <- getRgshhsMap("~/Downloads/gshhg-bin-2.3.7/gshhs_f.b", xlim = c(-81, -79), ylim = c(24, 26))
require(WGCNA)
require(data.table)
require(flashClust)
require(vegan)
require(qqman)
require(plyr)
require(dplyr)
#############
spp <- 'ssid' #ssid or mcav
ld.stat <- 'EM' #EM or pc (Pearson correlation)
setwd(paste0('~/Google Drive/Work/~Research/~Projects_Active/AdultJuv_Depth_FL/work/',spp))
load('LDsquare_datt_traits.RData')
if(ld.stat == 'pc'){rm(ldmat.sq.EM)} else if(ld.stat == 'EM'){rm(ldmat.sq.pc)}
#ngsLD is removing sites for some reason...
datt <- datt[,-which(!(colnames(datt) %in% names(mafs)))]
mindist=1000
#folding mafs
#here "minor" is defined as the lower frequency allele (as opposed to the non-reference allele)
#if the non-reference allele frequency is greater than 0.5 in the mafs file, we subtract from 1 to get the frequency of the "minor" allele
mafs.fold=mafs
mafs.fold[mafs.fold>0.5]=1-mafs.fold[mafs.fold>0.5]
chrom.tm=factor(sub("\\:.+","",sites))
pos.tm=as.numeric(as.character(sub(".+\\:","",sites)))
sites.order <- sites[order(as.numeric(chrom.tm), pos.tm)]
chrom=factor(sub("\\:.+","",sites.order))
pos=as.numeric(as.character(sub(".+\\:","",sites.order)))
#Create data frame of loci ordered by position within each chromosome
sites.df <- data.frame(sites = sites.order,
chrom = sub("\\:.+","",sites.order),
pos = as.numeric(as.character(sub(".+\\:","",sites.order))),
maf = mafs.fold[match(sites.order, names(mafs.fold))]) #which mafs dataset you choose here is consequential
sites.df$sites <- as.character(sites.df$sites)
#For de novo 2bRAD genomes, linkage filtering must account for SNPs on the same tag but there is no way of knowing the genomic distance between each tag
if(spp == 'ssid'){
tags.df <- data.frame(beg = seq(1,max(sites.df$pos), by=36), end = seq(36,max(sites.df$pos)+36, by=36))
tags <- c()
for(i in 1:nrow(sites.df)){tags <- c(tags, which(tags.df$beg <= sites.df$pos[i] & tags.df$end >= sites.df$pos[i]))}
sites.df$tag <- as.factor(tags)
sites.df <- sites.df %>% group_by(chrom,tag) %>% mutate(max.maf = max(maf))
prune.link <- colnames(datt)[which(sites.df$maf != sites.df$max.maf)]
} else if(spp == 'mcav'){
#Add column of distance between each site
sites.df <- sites.df %>% group_by(chrom) %>% mutate(diff = c(Inf, diff(pos)))
links <- which(sites.df$diff < mindist)
linkGrp <- function(x){
if(!is.numeric(x)) x <- as.numeric(x)
n <- length(x)
y <- x[-1] != x[-n] + 1
i <- c(which(y|is.na(y)),n)
list(
nsites = diff(c(0,i)), #number of sites in each linkage group
begin = x[head(c(0,i)+1,-1)] - 1 #first index of each linkage group
)
}
link.groups <- linkGrp(links)
#Retain the site within each "linkage" group with the highest allele frequency
#All others are filtered out of the dataset
prune.link <- c()
for(i in 1:length(link.groups$nsites)){
prune.link <- prune.link
link.grp <- seq(0,link.groups$nsites[i]) + link.groups$begin[i]
sites.grp <- sites.df[link.grp,]
max.maf <- which.max(sites.grp$maf)
prune.link <- c(prune.link, as.character(sites.grp$sites[-max.maf]))
}
}
#remove monomorphic sites (this should be NULL, all majority-heterozygote sites should be removed using HetMajorityProb.py script)
prune.mono <- names(which(apply(datt, 2, var, na.rm=TRUE) == 0))
prune.all <- c(prune.link, prune.mono)
message(length(prune.all)) #1465 (mcav), 2872 (ssid)
if(ld.stat == 'pc'){
ldmat.filt <- ldmat.sq.pc[-which(colnames(ldmat.sq.pc) %in% prune.all),-which(colnames(ldmat.sq.pc) %in% prune.all)]
} else if (ld.stat == 'EM'){
ldmat.filt <- ldmat.sq.EM[-which(colnames(ldmat.sq.EM) %in% prune.all),-which(colnames(ldmat.sq.EM) %in% prune.all)]
}
mafs.fold=mafs.fold[-which(names(mafs.fold) %in% prune.all)]
mafs=mafs[-which(names(mafs) %in% prune.all)]
datt=datt[,-which(colnames(datt) %in% prune.all)]
# Lowest power for which the the scale-free topology fit index reaches 0.90
power=1
ldmat.filt[ldmat.filt==Inf]=1
TOM = TOMsimilarity(ldmat.filt^power, TOMType="unsigned");
dissTOM = 1-TOM
# Call the hierarchical clustering function
geneTree = flashClust(as.dist(dissTOM), method = "average");
minModuleSize = 20;
dynamicMods = cutreeDynamic(dendro = geneTree, distM = dissTOM,
deepSplit = 2, pamRespectsDendro = FALSE,
minClusterSize = minModuleSize);
dynamicColors = labels2colors(dynamicMods)
data.frame(table(dynamicColors))
plotDendroAndColors(geneTree, dynamicColors, "Dynamic Tree Cut",
dendroLabels = FALSE, hang = 0,
addGuide = FALSE, guideHang = 0.05,
main = "Gene dendrogram and module colors")
dev.off()
plotDendroAndColors(geneTree, dynamicColors, "Dynamic Tree Cut",
dendroLabels = FALSE, hang = 0,
addGuide = FALSE, guideHang = 0.05,
main = "Gene dendrogram and module colors")
dev.off()
MEList = moduleEigengenes(datt, colors = dynamicColors)
MEs = MEList$eigengenes
MEs$MEgrey=NULL
# Calculate dissimilarity of module eigengenes
MEDiss = 1-cor(MEs);
METree = flashClust(as.dist(MEDiss), method = "average")
MEDissThres = 0 # in the first pass, set this to 0 - no merging (we want to see the module-traits heatmap first, then decide which modules are telling us the same story and better be merged)
#pdf("clusterModuleEigen.pdf")
plot(METree, main = "Clustering of module eigengenes",
xlab = "", sub = "")
# Plot the cut line into the dendrogram
abline(h=MEDissThres, col = "red")  # on 2nd pass: does this cut height meet your merging goals? If not, reset MEDissThres and replot
dev.off()
# Call an automatic merging function
merge = mergeCloseModules(datt, dynamicColors, cutHeight = MEDissThres, verbose = 3)
# The merged module colors
mergedColors = merge$colors;
# Eigengenes of the new merged modules:
mergedMEs = merge$newMEs
# plotting the fabulous ridiculogram
pdf(paste0(spp,"_geneModuleTree.pdf"), width = 7.5, height = 5.5)
plotDendroAndColors(geneTree,mergedColors,
dendroLabels = FALSE, hang = 0.0,
addGuide = F, guideHang = 0.05,lwd=0.6,main=paste0("merged at r = ", MEDissThres),autoColorHeight=F,colorHeight=0.15)
dev.off()
# Rename to moduleColors
moduleColors = mergedColors
# Construct numerical labels corresponding to the colors
colorOrder = c("grey", standardColors(50));
moduleLabels = match(moduleColors, colorOrder)-1;
MEs = mergedMEs;
MEs$MEgrey=NULL
# Calculate dissimilarity of module eigengenes
MEDiss = 1-cor(MEs);
# Cluster module eigengenes
METree = flashClust(as.dist(MEDiss), method = "average");
# Plot the result
#	sizeGrWindow(7, 6)
plot(METree, main = "mc_all",
xlab = "", sub = "")
dev.off()
# Define numbers of genes and samples
nGenes = ncol(datt);
nSamples = nrow(datt);
# Recalculate MEs with color labels
MEs0 = moduleEigengenes(datt, moduleColors)$eigengenes
MEs = orderMEs(MEs0)
MEs$MEgrey=NULL
# correlations of genes with eigengenes (same as signedkME output between MEs and datt, aka "module membership")
moduleGeneCor=cor(MEs,datt)
moduleGenePvalue = corPvalueStudent(moduleGeneCor, nSamples);
Traits=traits[,-c(1:3)]
Traits$isDA <- as.numeric(traits$site=="D" & traits$age=="A")
Traits$isDJ <- as.numeric(traits$site=="D" & traits$age=="J")
moduleTraitCor = cor(MEs, Traits, use = "p");
moduleTraitPvalue = corPvalueStudent(moduleTraitCor, nSamples);
head(MEs)
pdf(paste0(spp,"_MEheatmap.pdf"), width = 8, height = 4)
textMatrix = paste(signif(moduleTraitCor, 2), "\n(",
signif(moduleTraitPvalue, 1), ")", sep = "");
dim(textMatrix) = dim(moduleTraitCor)
# Display the correlation values within a heatmap plot
# Positive correlation indicates elevated frequency of derived (minor) alleles
# Negative correlation indicates elevated frequency of ancestral (major/reference) alleles
labeledHeatmap(Matrix = moduleTraitCor,
xLabels = names(Traits),
#    yLabels = names(MEs),
yLabels = sub("ME","",row.names(moduleTraitCor)),
ySymbols = row.names(moduleTraitCor),
colorLabels = FALSE,
colors = blueWhiteRed(50),
textMatrix = textMatrix,
setStdMargins = FALSE,
cex.text = 0.6,
#		cex.text = 0.0001,
zlim = c(-1,1),
main=paste0(spp,"_all"))
dev.off()
#	mtrows=row.names(moduleTraitCor)
print(data.frame(table(moduleColors))) # gives numbers of genes in each module
row.names(MEs)=row.names(traits)
colnames(MEs)=sub("ME","",colnames(MEs))
head(MEs)
geneModuleMembership = as.data.frame(signedKME(datt, MEs));
colnames(geneModuleMembership) = colnames(MEs)
save(datt,traits,Traits,MEs,METree,geneTree,moduleColors,moduleLabels,nSamples, nGenes, moduleGeneCor, moduleGenePvalue, moduleTraitPvalue, moduleTraitCor,geneModuleMembership,file=paste0(spp,"_wgcna_p1.RData"))
# Discriminant analysis of individuals based on module eigengenes
rr=rda(MEs,scale=F)
#biplot(rr, col=traits$k)
plot(rr$CA$u,pch=as.numeric(traits$k)-1, col="grey50", xlim=c(-0.25,0.2),ylim=c(-0.25,0.4))
arrowScale=6
mod.cols <- unique(moduleColors)
nmods=length(mod.cols)
arrows(rep(0,nmods),rep(0,nmods),rr$CA$v[c(1:nmods),1]/arrowScale,rr$CA$v[c(1:nmods),2]/arrowScale,length=0.1,lwd=2,col=mod.cols)
legend("bottomleft",pch=unique(as.numeric(traits$k)-1),legend=unique(traits$k),bty="n",title="cluster",cex=0.9)
dev.off()
conds2=cbind(traits,MEs)
conds2$k=factor(conds2$k)
pdf(paste0(spp,"_eigPopCluster.pdf"), width = 8, height = 6)
par(mfrow=c(2,3))
for(i in 1:nmods){plot(get(mod.cols[i])~k, conds2, main=mod.cols[i], mgp=c(2.1,1,0), ylab = "Eigengene")}
dev.off()
# Discriminant analysis of SNPs based on module membership/correlation
par(mfrow=c(1,1))
rr=rda(t(moduleGeneCor))
plot(rr$CA$eig)
arrowScale=20
axes2plot=c(1,2)
plot(rr$CA$u[,axes2plot], pch=16, cex=0.5, col=rgb(0,0,0,alpha=0.1), mgp=c(2.3,1,0), ylim = c(-0.05, 0.05))
arrows(rep(0,nmods), rep(0,nmods), rr$CA$v[c(1:nmods), axes2plot[1]]/arrowScale, rr$CA$v[c(1:nmods), axes2plot[2]]/arrowScale, length=0.1, lwd=2, col=mod.cols)
# -------------- Manhattan plot
require(qqman)
chrom.num <- as.factor(sub("\\:.+", "", rownames(geneModuleMembership)))
levels(chrom.num) <- seq(1:length(levels(chrom.num)))
chrom.num <- as.numeric(chrom.num)
par(mfrow=c(2,3))
#for(i in 1:nrow(moduleGenePvalue)){
#  mandf <- data.frame(SNP = seq(1, nGenes),
#                      CHR = chrom.num,
#                      BP = as.numeric(sub(".+\\:", "", rownames(geneModuleMembership))),
#                      P = -log(moduleGenePvalue[i,], 10))
#  manhattan(mandf, main = rownames(moduleGenePvalue)[i])
#}
dev.off()
setwd(paste0('~/Google Drive/Work/~Research/~Projects_Active/AdultJuv_Depth_FL/work/',spp))
load(paste0(spp,"_wgcna_p1_sep11_2019.RData"))
load(paste0(spp,"_wgcna_p1.RData"))
library(WGCNA)
library(plotrix) # for color.scale
length(moduleColors)
data.frame(table(moduleColors))
nGenes = ncol(datt);
nSamples = nrow(datt);
# names (colors) of the modules
modNames = names(MEs)
geneModuleMembership = as.data.frame(signedKME(datt, MEs)); # same as moduleGeneCor
MMPvalue = as.data.frame(corPvalueStudent(as.matrix(geneModuleMembership), nSamples)); # same as moduleGenePvalue
names(geneModuleMembership) = paste("MM", modNames, sep="");
names(MMPvalue) = paste("p.MM", modNames, sep="");
for(whichTrait in c("is1","is2","is3","is4","isA","isD","isO","isN")){
selTrait = as.data.frame(traits[,whichTrait]);
names(selTrait) = whichTrait
# correlation of genes to traits
# (positive value suggests membership to trait category is correlated with more derived alleles at that site)
geneTraitSignificance = as.data.frame(cor(datt, selTrait, use = "p"));
GSPvalue = as.data.frame(corPvalueStudent(as.matrix(geneTraitSignificance), nSamples));
names(geneTraitSignificance) = paste("GS.", names(selTrait), sep="");
names(GSPvalue) = paste("p.GS.", names(selTrait), sep="");
pdf(paste0(spp, "_geneSigModuleCorr_", whichTrait, ".pdf"), width = 7.5, height = 5.5)
par(mfrow=c(2,3))
counter=0
for(module in modNames){
counter=counter+1
if (counter>9) {
quartz()
par(mfrow=c(3,3))
counter=1
}
column = match(module, modNames);
moduleGenes = which(moduleColors==module );
moduleMafs = mafs[moduleColors==module]; #use mafs here instead of mafs.fold to represent frequency of derived allele
moduleGenes=moduleGenes[order(moduleMafs)]
length(moduleGenes)
moduleMafs=moduleMafs[order(moduleMafs)]
verboseScatterplot(abs(geneModuleMembership[moduleGenes, column]),
ylim=c(-1,1),xlim=c(0,1),
geneTraitSignificance[moduleGenes, 1],
#	abs(geneTraitSignificance[moduleGenes, 1]),
xlab = paste(module,"kME"),
ylab = paste("GS for", whichTrait),
#	col = rgb(0,0,0,alpha=0.1),
col=color.scale(moduleMafs,c(0,1),c(0.8,0),c(1,0),alpha=0.5),
pch=16,mgp=c(2.3,1,0)
)
abline(0,1,lty=3,col="grey50")
abline(0,-1,lty=3,col="grey50")
}
# plotting color scale bar
colscale=data.frame(cbind("color"=1,"maf"=seq(0,1,0.01)))
plot(color~maf,colscale,pch=16,cex=3,col=color.scale(seq(0,1,0.01),c(0,1),c(0.8,0),c(1,0)))
dev.off()
}
for(module in modNames){
pdf(paste0(spp, "_alleleDensity_", module, ".pdf"), width = 7.5, height = 5.5)
par(mfrow=c(3,3))
kmeCutoff=0.25
tt=traits[,c("is1","is2","is3","is4","isA","isD","isO","isN")]
modNames = names(MEs)
for(i in 1:length(tt)) {
selTrait = as.data.frame(tt[,i]);
whichTrait=names(tt)[i]
geneTraitSignificance = as.data.frame(cor(datt, selTrait, use = "p"));
moduleGenes = which(moduleColors==module);
#	plot(density(geneTraitSignificance[moduleGenes,1]),main=module,xlim=c(-1,1))
#retain top percentage of genes in specified module based on absolute value of module membership
gm=abs(geneModuleMembership[moduleGenes, paste("MM",module,sep="")])
moduleGenesTop=moduleGenes[which(gm>quantile(gm,1-kmeCutoff))]
#subset geno data by NON-module genes and samples associated with the trait
moduledatt0=datt[as.logical(tt[,whichTrait]),which(moduleColors!=module)]
#or subset geno data by module genes and samples NOT associated with the trait
#moduledatt0=datt[!as.logical(tt[,whichTrait]),moduleGenes]
#and calculate average number of derived alleles for each gene
meangt0=apply(moduledatt0,2,mean)
d0=density(meangt0)
d0$y=d0$y/max(d0$y)
plot(d0,col="cyan3",main=paste(module,":",whichTrait),xlab="average number of derived alleles",mgp=c(2.3,1,0),bty="n",yaxt="n",ylab="")
#	mtext(side=2,"Normalized density",cex=0.5)
#subset geno data by module genes and samples that ARE associated with the trait
moduledatt=datt[as.logical(tt[,whichTrait]),moduleGenes]
meangt=apply(moduledatt,2,mean)
d1=density(meangt)
d1$y=d1$y/max(d1$y)
lines(d1,col="goldenrod")
# subset geno data by module genes with top 25% module membership and samples that ARE associated with the trait
moduledatt=datt[as.logical(tt[,whichTrait]),moduleGenesTop]
meangt=apply(moduledatt,2,mean)
d1=density(meangt)
d1$y=d1$y/max(d1$y)
lines(d1,col="red")
#	abline(v=0,lty=3)
}
plot(d0,col="grey50",type="n",xlab="",mgp=c(2.3,1,0),bty="n",yaxt="n",xaxt="n",ylab="",main="")
legend("topright",lty=1,col=c("red","goldenrod","cyan3"),lwd=2,legend=c("top 25% kME genes","all genes in module","genes not in module"),bty="n")
dev.off()
}
setwd(paste0('~/Google Drive/Work/~Research/~Projects_Active/AdultJuv_Depth_FL/work/',spp))
ll=load(paste0(spp,"_wgcna_p1.RData"))
genes=read.table("mcav_genes_clean.txt")
names(genes)=c("contig","start","end","gene")
genes$contig=as.character(genes$contig)
modNames = names(MEs)
# Generate list of each module's SNPs with associated contig, position and log10 p-value of module membership
coords=c()
