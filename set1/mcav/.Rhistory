library(dplyr)
library(ggplot2)
library(ggmap)
detach("package:ggmap", unload=TRUE)
library(rgeos)
library(rgdal)
library(maps)
library(mapdata)
library(maptools)
library(scales) #for transparency
library(GISTools)
library(devtools)
install.packages('png')
library(ggmap)
detach("package:ggmap", unload=TRUE)
library(rgeos)
library(rgdal)
library(maps)
library(mapdata)
library(maptools)
library(scales) #for transparency
library(GISTools)
library(devtools)
if (!rgeosStatus()) gpclibPermit()
gshhs.f.b <- "~/Downloads/gshhs_f.b"
sf1 <- getRgshhsMap(gshhs.f.b, xlim = c(-89, -87), ylim = c(16, 18)) %>%
fortify()
#import GSHHS coastline
require(maptools)
Coast<-getRgshhsMap('~/Downloads/gshhg-bin-2.3.7/gshhs_f.b')
require(ncdf4)
require(raster)
require(fields)
require(zoo)
require(xts)
Sys.setenv(TZ="UTC")
Xmin<- -81.5 + 360
Xmax<- -78.5 + 360
Ymin<-23.5
Ymax<-26.5
start.date<-"1984/01/01"
end.date<-"2015/12/31"
start.date<-as.POSIXct(start.date,tz="UTC")
end.date<-as.POSIXct(end.date,tz="UTC")
#export folder
export.folder<-"~/Google Drive/Work/~Research/~Projects_Active/CaribCoring_NSF/Data/CloudData/"
#raw data file
raw.data<-"~/Google Drive/Work/~Research/~Projects_Active/CaribCoring_NSF/Data/CloudData/ISCCP-H_Aggregation_Basic_Gridded_Monthly_(HGM)_best.ncd.nc"
#function to find the probability given the correlation coeficient (R) and the number of observations (N)
r.to.sign<-function(R,N)
{
T<-R/(sqrt((1-(R^2))/(N-2)))
return((1-pt(T,df=N-2))*2)
}
#function to calculate the effective size of an autocorrelated dataseries (quenouille correction)
eff.sample<-function(corr.coef,N)
{
return(round(N*((1-corr.coef)/(1+corr.coef)),0))
}
#get dims from netcdf
ncdf<-nc_open(raw.data)
nc.lats<-ncdf$dim$lat$vals
nc.longs<-ncdf$dim$lon$vals
nc.longs[nc.longs>180]<- nc.longs[nc.longs>180]-360
raw.times<-ncdf$dim$time$vals #times are seconds after the "start of time"
start.of.time<-ncdf$dim$time$units #here's what they consider the start of time
nc.times<-as.POSIXct(raw.times*3600,tz="UTC", gsub("hours since ","",start.of.time))
nc_close(ncdf)
#open netcdf as rasterbrick and clip it
data.brick<-brick(raw.data, lvar = 4, varname = 'cldamt')
data.brick<-crop(data.brick, extent(Xmin,Xmax,Ymin,Ymax))
#restrict temporally
Tstart.index<-min(which(difftime(nc.times,start.date)>0))
Tend.index<-min(which(difftime(nc.times,end.date)>0))
data.brick<-data.brick[[Tstart.index:Tend.index]]
site.raw.data<-read.csv("~/Google Drive/Work/~Research/~Projects_Active/CaribCoring_NSF/Data/Mapping/SamplingSites/SitePoints.csv")
site.raw.data<-site.raw.data[site.raw.data$Region=='Florida',]
#long and lat as a two column matrix
LongLat<-cbind(site.raw.data$Longitude + 360,site.raw.data$Latitude)
#converting sampling points to spatial points
sampling.points<-SpatialPoints(LongLat)
#create projection
lat.long.proj<-"+proj=longlat +ellps=WGS84 +datum=WGS84"
#apply projection to points
projection(sampling.points)<-lat.long.proj
first.day<-data.brick[[1]]
plot(first.day, col=tim.colors(255,1), xlab="Long", ylab="Lat", main=paste0("Cloudiness of Florida Keys on ",start.date), colNA="grey", cex.main=0.8, cex.axis=0.8, cex.lab=0.7)
#some pixels fall on land. Finding nearest valid pixels
#pixels coordinates
pixel.coordinates<-coordinates(first.day)
#delete coordinates where data is missing (NA)
#first, extract data for all pixels
data<-extract(first.day,pixel.coordinates)
#now check which ones are NA and delete those entries from the "pixel.coordinates" list
pixel.coordinates<-pixel.coordinates[!is.na(data),]
#create new sampling points, shifted to the center of the pixel they fall on, or to the near pixel in case that one is NA
sampling.points.shifted<-vector()
#loop cycling through sampling points
for (p in 1:length(sampling.points)) {
#geodesic distances
distances<-rdist.earth(coordinates(sampling.points[p]),pixel.coordinates,miles=FALSE)
#coordinates of the pixel that is at shorter distance
shifted.coordinates<-pixel.coordinates[which.min(distances),]
#assigning those coordinates to a new item
cmd<-paste0("sp.",p,"<-shifted.coordinates")
eval(parse(text=cmd))
#points(pt[1],pt[2])
cmd<-paste0("sampling.points.shifted<-rbind(sampling.points.shifted,sp.",p,")")
eval(parse(text=cmd))
}
coastline<-getRgshhsMap("~/Google\ Drive/Work/~Research/~Projects_Active/CaribCoring_NSF/Data/Mapping/gshhg-bin-2.3.6/gshhs_f.b", xlim=c(extent(data.brick)[1], extent(data.brick)[2]), ylim=c(extent(data.brick)[3], extent(data.brick)[4]), level=4, shift=TRUE, no.clip=FALSE)
gshhs.f.b <- "~/Downloads/gshhs_f.b"
sf1 <- getRgshhsMap(gshhs.f.b, xlim = c(-89, -87), ylim = c(16, 18))
#import and plot GSHHS coastline
require(maptools)
coastline<-getRgshhsMap('~/Downloads/gshhs_f.b')
coastline<-getRgshhsMap('~/Downloads/gshhs_f.b', xlim = c(-81,-79), ylim = c(24, 26))
read.table('~/Downloads/gshhs_f.b')
read.table('~/Downloads/gshhs_f.b')
read.table('~/Downloads/gshhs_f.txt')
read.table('~/Downloads/gshhg-bin-2.3.7/gshhs_f.b')
gshhs.f.b <- "~/Downloads/gshhg-bin-2.3.7/gshhs_f.b"
sf1 <- getRgshhsMap(gshhs.f.b, xlim = c(-89, -87), ylim = c(16, 18))
head(gshhs.f.b)
head(read.table(gshhs.f.b))
gshhs.f.b <- "~/Downloads/gshhg-bin-2.3.0/gshhs_f.b"
sf1 <- getRgshhsMap(gshhs.f.b, xlim = c(-89, -87), ylim = c(16, 18))
install.packages('sf')
require(sf)
coastline<-st_read("~/Downloads/gshhg-shp-2.3.7/GSHHS_shp/f/GSHHS_f_L4.shp")
st_crs(coastline)
coastline <- file("~/Downloads/gshhg-bin-2.3.7/gshhs_f.b", "rb")
test <- getRgshhsMap(coastline)
test <- getRgshhsMap("~/Downloads/gshhg-bin-2.3.7/gshhs_f.b", xlim = c(-81, -79), ylim = c(24, 26))
library(RSQLite)
library(DBI)
# Create an ephemeral in-memory RSQLite database
con <- dbConnect(RSQLite::SQLite(), ":memory:")
dbListTables(con)
dbWriteTable(con, "mtcars", mtcars)
dbListTables(con)
dbWriteTable(con, "test", mtcars)
dbListTables(con)
dbListFields(con, "test")
setwd("~/Google Drive/Work/~Research/~Projects_Active/AdultJuv_Depth_FL/Molecular/TagSeq/Picogreen/")
inFile="102519_Plate2.csv"
pg=read.csv(inFile,header=F)
names(pg)=c("well","fluo")
pg[,2]=pg[,2]-pg[pg$well=="A1",2]
pg=pg[pg$well!="A1",]
pg=pg[pg$fluo>0,]
pg$fluo=log(pg$fluo,10)
cal=pg[grep("[ABCDEFGH]1$",pg$well),]
cal$conc=log(c(100,100/3,100/3^2,100/3^3,100/3^4,100/3^5,100/3^6),10)
str(cal)
plot(conc~fluo,cal,xaxt="n",yaxt="n",bty="n",xlab="fluorescence",ylab="DNA concentration (ng/ul)",mgp=c(2.1,1,0))
ll=lm(conc~fluo,cal)
abline(ll,col="red")
axis(1,at=cal$fluo,labels=round(10^(cal$fluo),0))
axis(2,at=cal$conc,labels=signif(10^(cal$conc),2))
sam=pg[-grep("[ABCDEFGH]1$",pg$well),]
sam=sam[sam$fluo>0,]
sam$conc=predict(ll,newdata=data.frame(fluo=sam$fluo))
points(conc~fluo,sam,col="cyan3")
legend("topleft",col=c("black","cyan3"),pch=1,c("calibration","samples"),bty="n")
sam$conc=round(10^sam$conc,1)
sam$fluo=NULL
names(sam)[2]="ng/ul"
sam
write.csv(sam,file=paste("res_",inFile,sep=""),quote=F,row.names=F)
setwd("~/Google Drive/Work/~Research/~Projects_Active/AdultJuv_Depth_FL/Molecular/TagSeq/Picogreen/")
inFile="102819_Plate3.csv"
pg=read.csv(inFile,header=F)
names(pg)=c("well","fluo")
pg[,2]=pg[,2]-pg[pg$well=="A1",2]
pg=pg[pg$well!="A1",]
pg=pg[pg$fluo>0,]
pg$fluo=log(pg$fluo,10)
cal=pg[grep("[ABCDEFGH]1$",pg$well),]
cal$conc=log(c(100,100/3,100/3^2,100/3^3,100/3^4,100/3^5,100/3^6),10)
str(cal)
plot(conc~fluo,cal,xaxt="n",yaxt="n",bty="n",xlab="fluorescence",ylab="DNA concentration (ng/ul)",mgp=c(2.1,1,0))
ll=lm(conc~fluo,cal)
abline(ll,col="red")
axis(1,at=cal$fluo,labels=round(10^(cal$fluo),0))
axis(2,at=cal$conc,labels=signif(10^(cal$conc),2))
sam=pg[-grep("[ABCDEFGH]1$",pg$well),]
sam=sam[sam$fluo>0,]
sam$conc=predict(ll,newdata=data.frame(fluo=sam$fluo))
points(conc~fluo,sam,col="cyan3")
legend("topleft",col=c("black","cyan3"),pch=1,c("calibration","samples"),bty="n")
sam$conc=round(10^sam$conc,1)
sam$fluo=NULL
names(sam)[2]="ng/ul"
sam
write.csv(sam,file=paste("res_",inFile,sep=""),quote=F,row.names=F)
inFile="103019_Samples10_14_17.csv"
setwd("~/Google Drive/Work/~Research/~Projects_Active/AdultJuv_Depth_FL/Molecular/TagSeq/Picogreen/")
inFile="103019_Samples10_14_17.csv"
pg=read.csv(inFile,header=F)
names(pg)=c("well","fluo")
pg[,2]=pg[,2]-pg[pg$well=="A8",2]
pg=pg[pg$well!="A8",]
pg=pg[pg$fluo>0,]
pg$fluo=log(pg$fluo,10)
cal=pg[grep("[ABCDEFGH]8$",pg$well),]
cal$conc=log(c(100,100/3,100/3^2,100/3^3,100/3^4,100/3^5,100/3^6),10)
str(cal)
plot(conc~fluo,cal,xaxt="n",yaxt="n",bty="n",xlab="fluorescence",ylab="DNA concentration (ng/ul)",mgp=c(2.1,1,0))
ll=lm(conc~fluo,cal)
abline(ll,col="red")
axis(1,at=cal$fluo,labels=round(10^(cal$fluo),0))
axis(2,at=cal$conc,labels=signif(10^(cal$conc),2))
sam=pg[-grep("[ABCDEFGH]8$",pg$well),]
sam=sam[sam$fluo>0,]
sam$conc=predict(ll,newdata=data.frame(fluo=sam$fluo))
points(conc~fluo,sam,col="cyan3")
legend("topleft",col=c("black","cyan3"),pch=1,c("calibration","samples"),bty="n")
sam$conc=round(10^sam$conc,1)
sam$fluo=NULL
names(sam)[2]="ng/ul"
sam
write.csv(sam,file=paste("res_",inFile,sep=""),quote=F,row.names=F)
bams <- read.table('~/Google Drive/Work/~Research/~Projects_Active/AdultJuv_Depth_FL/work/mcav/bams_noclones')
View(bams)
bams[,1] <- gsub("\\.*","",bams[,1])
View(bams)
bams <- read.table('~/Google Drive/Work/~Research/~Projects_Active/AdultJuv_Depth_FL/work/mcav/bams_noclones')
bams[,1] <- gsub("\\..*","",bams[,1])
View(bams)
bams$id <- seq(0,104)
bams$id <- paste0('ind',bams$id)
fasta <- read.table('~/Google Drive/Work/~Research/~Projects_Active/AdultJuv_Depth_FL/work/mcav/BEAST_tree/mc1_tree.fasta', header = F)
View(fasta)
seq(2, nrow(fasta), by=2)
samples <- c()
i=2
fasta[i,1]
i=1
fasta[i,1]
which(bams$V1 == gsub(">","",as.character(fasta[i,1])))
as.character(fasta[i,1])
gsub(">","",as.character(fasta[i,1]))
bams$V1
bams$id
replace.name <- which(bams$id == gsub(">","",as.character(fasta[i,1])))
i=3
replace.name <- which(bams$id == gsub(">","",as.character(fasta[i,1])))
replace.name
for(i in seq(1, nrow(fasta), by=2)) {fasta[i,1] <- bams$V1[which(bams$id == gsub(">","",as.character(fasta[i,1])))]}
fasta <- read.table('~/Google Drive/Work/~Research/~Projects_Active/AdultJuv_Depth_FL/work/mcav/BEAST_tree/mc1_tree.fasta', header = F)
fasta <- as.character(read.table('~/Google Drive/Work/~Research/~Projects_Active/AdultJuv_Depth_FL/work/mcav/BEAST_tree/mc1_tree.fasta', header = F))
fasta <- as.characterread.table('~/Google Drive/Work/~Research/~Projects_Active/AdultJuv_Depth_FL/work/mcav/BEAST_tree/mc1_tree.fasta', header = F)
fasta <- read.table('~/Google Drive/Work/~Research/~Projects_Active/AdultJuv_Depth_FL/work/mcav/BEAST_tree/mc1_tree.fasta', header = F)
fasta[,1] <- as.character(fasta[,1])
View(fasta)
for(i in seq(1, nrow(fasta), by=2)) {fasta[i,1] <- bams$V1[which(bams$id == gsub(">","",as.character(fasta[i,1])))]}
View(fasta)
fasta <- read.table('~/Google Drive/Work/~Research/~Projects_Active/AdultJuv_Depth_FL/work/mcav/BEAST_tree/mc1_tree.fasta', header = F)
fasta[,1] <- as.character(fasta[,1])
for(i in seq(1, nrow(fasta), by=2)) {fasta[i,1] <- paste0(">",bams$V1[which(bams$id == gsub(">","",as.character(fasta[i,1])))])}
View(fasta)
setwd('~/Google Drive/Work/~Research/~Projects_Active/AdultJuv_Depth_FL/work/mcav/BEAST_tree/')
write.table('mc1_tree_relabel.fasta', row.names = F, col.names = F)
write.table(fasta, 'mc1_tree_relabel.fasta', row.names = F, col.names = F)
write.table(fasta, 'mc1_tree_relabel.fasta', row.names = F, col.names = F, quote = F)
library(sp)
library(xts)
library(scales)
library(Rmisc)
library(reshape2)
library(mgcv)
library(splines)
library(vegan)
library(MuMIn)
library(MASS)
library(nlme)
library(dplyr)
library(ggplot2)
library(gganimate)
library(gifski)
library(transformr)
setwd('~/Google Drive/Work/~Research/~Projects_Active/CaribCoring_NSF/Data/')
stat.scale <- function(x) {(x - mean(x, na.rm = T))/(2*sd(x, na.rm = T))}
#source('~/Google Drive/Work/~Research/R Scripts/Coring/GrowthSST_dataProcess.R')
source('~/Google Drive/Work/~Research/R Scripts/Coring/summaryPlotFunctions.R')
load('growthSST.Rdata')
growth.sst <- growth.noNA
sst.dataset <- 'oi' # Select SST data source
for(stat in 1:length(annual.data.lol)){
if(!is.null(annual.data.lol[[stat]][[sst.dataset]])) {
df.add <- merge(growth.sst, annual.data.lol[[stat]][[sst.dataset]], sort = F)
growth.sst <- df.add
} else {next}
}
growth.sst$annual.max.all <- as.numeric(levels(growth.sst$annual.max.all))[growth.sst$annual.max.all]; growth.sst$annual.min.all <- as.numeric(levels(growth.sst$annual.min.all))[growth.sst$annual.min.all]
if(sst.dataset != 'hadi') {growth.sst <- growth.sst %>% group_by(site) %>% mutate(mean.weekly.range = mean(annual.weekly.range.all))}
## Site by year plot
#SST
ggplot()+
theme_bw()+
geom_point(data = growth.sst, aes(x=Year, y=summer.mean.all, group=site, color=region), size = 0.2, alpha = 0.2)+
geom_smooth(data = growth.sst, aes(x=Year, y=summer.mean.all, group=region, color=region), method = 'loess')+
facet_grid(.~region)
# GROWTH SUMMARY PLOTS
growth.plot.data <- read.csv('./Growth/Growth_allRegions.csv')
summary.df <- summarySE(growth.plot.data, measurevar = c('linext'), groupvars = c('region', 'spp'), na.rm = T)
View(summary.df)
summary.df <- summarySE(growth.plot.data, measurevar = c('linext'), groupvars = c('region', 'spp', 'Year'), na.rm = T)
View(summary.df)
ggplot()+
theme_bw()+
geom_line(data = summary.df, aes(x=Year, y=sd, color=region))
ggplot(data = summary.df, aes(x=Year, y=sd, color=region))+
theme_bw()+
geom_point()+
geom_smooth(method = 'lm')
ggplot(data = summary.df, aes(x=Year, y=sd, color=region))+
theme_bw()+
geom_point()+
geom_smooth(method = 'gam')
ggplot(data = summary.df, aes(x=Year, y=se, color=region))+
theme_bw()+
geom_point()+
geom_smooth(method = 'gam')
summary.df <- subset(summary.df, N >= 10)
ggplot(data = summary.df, aes(x=Year, y=se, color=region))+
theme_bw()+
geom_point()+
geom_smooth(method = 'gam')
ggplot(data = summary.df, aes(x=Year, y=se, color=region))+
theme_bw()+
geom_point()+
geom_smooth(method = 'loess')
ggplot(data = summary.df, aes(x=Year, y=sd, color=region))+
theme_bw()+
geom_point()+
geom_smooth(method = 'loess')
summary.df <- subset(summary.df, N >= 20)
ggplot(data = summary.df, aes(x=Year, y=sd, color=region))+
theme_bw()+
geom_point()+
geom_smooth(method = 'loess')
ggplot(data = summary.df, aes(x=Year, y=se, color=region))+
theme_bw()+
geom_point()+
geom_smooth(method = 'loess')
install.packages('strucchange')
library(strucchange)
require(WGCNA)
require(data.table)
require(flashClust)
require(vegan)
require(qqman)
require(plyr)
require(dplyr)
#############
spp <- 'mcav' #ssid or mcav
ld.stat <- 'rEM' #rEM or DEM (r2 or D calculated with EM algorithm)
#############
setwd(paste0('~/Google Drive/Work/~Research/~Projects_Active/AdultJuv_Depth_FL/work/',spp))
load('LDsquare_datt_traits.RData')
if(ld.stat == 'rEM'){rm(ldmat.sq.DEM)} else if(ld.stat == 'DEM'){rm(ldmat.sq.rEM)}
#ngsLD is removing sites for some reason...
datt <- datt[,-which(!(colnames(datt) %in% names(mafs)))]
setwd(paste0('~/Google Drive/Work/~Research/~Projects_Active/AdultJuv_Depth_FL/work/',spp))
load(paste0("./wgcna_",ld.stat,"/",spp,"_wgcna_p1.RData"))
mindist=1000
#folding mafs
#here "minor" is defined as the lower frequency allele (as opposed to the non-reference allele)
#if the non-reference allele frequency is greater than 0.5 in the mafs file, we subtract from 1 to get the frequency of the "minor" allele
mafs.fold=mafs
mafs.fold[mafs.fold>0.5]=1-mafs.fold[mafs.fold>0.5]
chrom.tm=factor(sub("\\:.+","",sites))
pos.tm=as.numeric(as.character(sub(".+\\:","",sites)))
sites.order <- sites[order(as.numeric(chrom.tm), pos.tm)]
chrom=factor(sub("\\:.+","",sites.order))
pos=as.numeric(as.character(sub(".+\\:","",sites.order)))
#Create data frame of loci ordered by position within each chromosome
sites.df <- data.frame(sites = sites.order,
chrom = sub("\\:.+","",sites.order),
pos = as.numeric(as.character(sub(".+\\:","",sites.order))),
maf = mafs.fold[match(sites.order, names(mafs.fold))]) #which mafs dataset you choose here is consequential
sites.df$sites <- as.character(sites.df$sites)
#For de novo 2bRAD genomes, linkage filtering must account for SNPs on the same tag but there is no way of knowing the genomic distance between each tag
if(spp == 'ssid'){
tags.df <- data.frame(beg = seq(1,max(sites.df$pos), by=36), end = seq(36,max(sites.df$pos)+36, by=36))
tags <- c()
for(i in 1:nrow(sites.df)){tags <- c(tags, which(tags.df$beg <= sites.df$pos[i] & tags.df$end >= sites.df$pos[i]))}
sites.df$tag <- as.factor(tags)
sites.df <- sites.df %>% group_by(chrom,tag) %>% mutate(max.maf = max(maf))
prune.link <- colnames(datt)[which(sites.df$maf != sites.df$max.maf)]
} else if(spp == 'mcav'){
#Add column of distance between each site
sites.df <- sites.df %>% group_by(chrom) %>% mutate(diff = c(Inf, diff(pos)))
links <- which(sites.df$diff < mindist)
linkGrp <- function(x){
if(!is.numeric(x)) x <- as.numeric(x)
n <- length(x)
y <- x[-1] != x[-n] + 1
i <- c(which(y|is.na(y)),n)
list(
nsites = diff(c(0,i)), #number of sites in each linkage group
begin = x[head(c(0,i)+1,-1)] - 1 #first index of each linkage group
)
}
link.groups <- linkGrp(links)
#Retain the site within each "linkage" group with the highest allele frequency
#All others are filtered out of the dataset
prune.link <- c()
for(i in 1:length(link.groups$nsites)){
prune.link <- prune.link
link.grp <- seq(0,link.groups$nsites[i]) + link.groups$begin[i]
sites.grp <- sites.df[link.grp,]
max.maf <- which.max(sites.grp$maf)
prune.link <- c(prune.link, as.character(sites.grp$sites[-max.maf]))
}
}
#remove monomorphic sites (this should be NULL, all majority-heterozygote sites should be removed using HetMajorityProb.py script)
prune.mono <- names(which(apply(datt, 2, var, na.rm=TRUE) == 0))
prune.all <- c(prune.link, prune.mono)
message(length(prune.all)) #1465 (mcav), 2872 (ssid)
if(ld.stat == 'DEM'){
ldmat.filt <- ldmat.sq.DEM[-which(colnames(ldmat.sq.DEM) %in% prune.all),-which(colnames(ldmat.sq.DEM) %in% prune.all)]
} else if (ld.stat == 'rEM'){
ldmat.filt <- ldmat.sq.rEM[-which(colnames(ldmat.sq.rEM) %in% prune.all),-which(colnames(ldmat.sq.rEM) %in% prune.all)]
}
mafs.fold=mafs.fold[-which(names(mafs.fold) %in% prune.all)]
mafs=mafs[-which(names(mafs) %in% prune.all)]
datt=datt[,-which(colnames(datt) %in% prune.all)]
#--------------- WGCNA
# # TOM and initial clustering
# powers = c(c(1:10), seq(from = 12, to=20, by=2))
# sft = pickSoftThreshold(datt, powerVector = powers, verbose = 5)
#
# # Plot the results:
# sizeGrWindow(9, 5)
# par(mfrow = c(1,2));
# cex1 = 0.9;
# # Scale-free topology fit index as a function of the soft-thresholding power
# plot(sft$fitIndices[,1], -sign(sft$fitIndices[,3])*sft$fitIndices[,2],
#      xlab="Soft Threshold (power)",ylab="Scale Free Topology Model Fit,signed R^2",type="n",
#      main = paste("Scale independence"));
# text(sft$fitIndices[,1], -sign(sft$fitIndices[,3])*sft$fitIndices[,2],
#      labels=powers,cex=cex1,col="red");
# # this line corresponds to using an R^2 cut-off of h
# abline(h=0.90,col="red")
# # Mean connectivity as a function of the soft-thresholding power
# plot(sft$fitIndices[,1], sft$fitIndices[,5],
#      xlab="Soft Threshold (power)",ylab="Mean Connectivity", type="n",
#      main = paste("Mean connectivity"))
# text(sft$fitIndices[,1], sft$fitIndices[,5], labels=powers, cex=cex1,col="red")
# Lowest power for which the the scale-free topology fit index reaches 0.90
power=1
ldmat.filt[ldmat.filt==Inf]=1
TOM = TOMsimilarity(ldmat.filt^power, TOMType="unsigned");
dissTOM = 1-TOM
# Call the hierarchical clustering function
geneTree = flashClust(as.dist(dissTOM), method = "average");
minModuleSize = 20;
dynamicMods = cutreeDynamic(dendro = geneTree, distM = dissTOM,
deepSplit = 2, pamRespectsDendro = FALSE,
minClusterSize = minModuleSize);
dynamicColors = labels2colors(dynamicMods)
data.frame(table(dynamicColors))
# Label 0 (grey) represents unassigned genes
1832+287+3410+266
5795+1465
message(length(prune.all)) #1465 (mcav), 2872 (ssid)
3410/7260
Traits=traits[,-c(1:3)]
Traits$isDA <- as.numeric(traits$site=="D" & traits$age=="A")
Traits$isDJ <- as.numeric(traits$site=="D" & traits$age=="J")
moduleTraitCor = cor(MEs, Traits, use = "p");
moduleTraitPvalue = corPvalueStudent(moduleTraitCor, nSamples);
head(MEs)
pdf(paste0("./wgcna_",ld.stat,"/",spp,"_MEheatmap.pdf"), width = 8, height = 4)
textMatrix = paste(signif(moduleTraitCor, 2), "\n(",
signif(moduleTraitPvalue, 1), ")", sep = "");
dim(textMatrix) = dim(moduleTraitCor)
# Display the correlation values within a heatmap plot
# Positive correlation indicates elevated frequency of derived (minor) alleles
# Negative correlation indicates elevated frequency of ancestral (major/reference) alleles
labeledHeatmap(Matrix = moduleTraitCor,
xLabels = names(Traits),
#    yLabels = names(MEs),
yLabels = sub("ME","",row.names(moduleTraitCor)),
ySymbols = row.names(moduleTraitCor),
colorLabels = FALSE,
colors = blueWhiteRed(50),
textMatrix = textMatrix,
setStdMargins = FALSE,
cex.text = 0.6,
#		cex.text = 0.0001,
zlim = c(-1,1),
main=paste0(spp,"_all"))
# Display the correlation values within a heatmap plot
# Positive correlation indicates elevated frequency of derived (minor) alleles
# Negative correlation indicates elevated frequency of ancestral (major/reference) alleles
labeledHeatmap(Matrix = moduleTraitCor,
xLabels = names(Traits),
#    yLabels = names(MEs),
yLabels = sub("ME","",row.names(moduleTraitCor)),
ySymbols = row.names(moduleTraitCor),
colorLabels = FALSE,
colors = blueWhiteRed(50),
textMatrix = textMatrix,
setStdMargins = FALSE,
cex.text = 0.6,
#		cex.text = 0.0001,
zlim = c(-1,1),
main=paste0(spp,"_all"))
